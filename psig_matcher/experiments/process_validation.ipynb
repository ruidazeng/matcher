{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from line_profiler import LineProfiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "import scipy.stats as st\n",
    "from statistics import NormalDist\n",
    "from scipy.stats import multivariate_normal as mvn\n",
    "import time\n",
    "\n",
    "@dataclass\n",
    "class NormalDistribution:\n",
    "    mean: float\n",
    "    std: float\n",
    "    \n",
    "@dataclass\n",
    "class Part:\n",
    "    \n",
    "    type: str\n",
    "    sub_part_name: str\n",
    "    sensor: str\n",
    "    signals: List   # Signal is numpy array of (500,3) with [frequency, Z, X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_part_data(part_type: str) -> List[Part]:\n",
    "    \n",
    "    parts = []\n",
    "    for part_dir in os.listdir(f'psig_matcher/data/{part_type}'):\n",
    "        \n",
    "        sensor = part_dir[1:]\n",
    "        measurement_files = glob.glob(f'psig_matcher/data/{part_type}/{part_dir}/*.npy')\n",
    "        measurements = [np.load(f) for f in measurement_files]\n",
    "        parts.append(Part(part_type, part_dir, sensor, measurements))\n",
    "    \n",
    "    return parts\n",
    "\n",
    "con_parts = load_part_data('CON')\n",
    "#conlid_parts = load_part_data('CONLID') # Need to handle the damage files\n",
    "lid_parts = load_part_data('LID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "\n",
    "\n",
    "def limit_deminsionality(parts: List[Part], frequeny_indexes: List[int]) -> List[Part]:\n",
    "    \"\"\"Use only a subset of the frequencies for the analysis. This effectivley transforms the \n",
    "    500 dimension multivariant distribution to a n-dimentional distribution where n is the\n",
    "    length of the frequency_indexes.\n",
    "    \n",
    "    Further, this assumes use of the X axis\"\"\"\n",
    "    \n",
    "    return [\n",
    "        dataclasses.replace(part, signals=[[signal[index][1] for index in frequeny_indexes] for signal in part.signals])\n",
    "        for part in parts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_normal_dist(x: List[float], confidence: float) -> NormalDistribution:\n",
    "    \"\"\"Estimate the normal distribution for the given data.\n",
    "    This is done using: https://handbook-5-1.cochrane.org/chapter_7/7_7_3_2_obtaining_standard_deviations_from_standard_errors_and.htm#:~:text=The%20standard%20deviation%20for%20each,should%20be%20replaced%20by%205.15.\n",
    "    \n",
    "    TODO (henry): I'm not sure this is correct.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use T distribution for small sample sizes\n",
    "    if len(x) < 30:\n",
    "        lower, upper = st.t.interval(confidence, len(x)-1, loc=np.mean(x), scale=st.sem(x))\n",
    "        t_value = st.t.ppf(confidence, len(x)-1)\n",
    "        std = np.sqrt(len(x))*(upper-lower)*t_value\n",
    "    \n",
    "    # Use normal distribution for larger sample sizes\n",
    "    else:\n",
    "        lower, upper = st.norm.interval(confidence, loc=np.mean(x), scale=st.sem(x))\n",
    "        z_value = st.norm.ppf(confidence)\n",
    "        std = np.sqrt(len(x))*(upper-lower)*z_value\n",
    "    \n",
    "    return NormalDistribution(np.mean(x, axis=0), std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concrete_normal_dist(x: List[float]) -> NormalDistribution:\n",
    "    return NormalDistribution(np.mean(x, axis=0), np.std(x, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_single_distributions(pdfs: List[NormalDistribution], labels: List[str], title: str):\n",
    "    \"\"\" pdfs is a list of tuples of (mean, std) for each distribution.\"\"\"\n",
    "    \n",
    "    for pdf, label in zip(pdfs, labels):\n",
    "        \n",
    "        x = np.linspace(pdf.mean - 3* pdf.std, pdf.mean + 3* pdf.std, 100)\n",
    "        plt.plot(x, norm.pdf(x, pdf.mean, pdf.std), label=label)\n",
    "    \n",
    "    plt.legend(loc='best')\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_overlap(normal_d_1: NormalDistribution, normal_d_2: NormalDistribution) -> float:\n",
    "    \"\"\"Finds the overlap between two distributions.\"\"\"\n",
    "    \n",
    "    return NormalDist(mu=normal_d_1.mean, sigma=normal_d_1.std).overlap(NormalDist(mu=normal_d_2.mean, sigma=normal_d_2.std))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_overlap_of_set(pdfs: List[NormalDistribution]) -> float:\n",
    "    \"\"\"Finds the overlap between a set of distributions.\"\"\"\n",
    "    \n",
    "    overlaps = []\n",
    "    for i in range(len(pdfs)):\n",
    "        for k in range(i+1, len(pdfs)):\n",
    "            # Currently this method redundantly counts overlaps that may have already been accounted for. \n",
    "            # If A and B overlap on the edge of B, but C also overlaps on the edge of B, we're double\n",
    "            # counting that overlap. Maybe we want to do this? \n",
    "            overlaps.append(find_overlap(pdfs[i], pdfs[k]))\n",
    "    \n",
    "    return np.mean(overlaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_1d_analysis(parts: List[Part]):\n",
    "    \n",
    "    single_d_parts = limit_deminsionality(parts, [0])\n",
    "    print(single_d_parts[0].signals)\n",
    "    pdfs = [estimate_normal_dist(part.signals, 0.95) for part in single_d_parts]\n",
    "    plot_single_distributions(pdfs, [f\"{part.type} - {part.sub_part_name}\" for part in single_d_parts], f'1D Analysis - Estimated Confidence at 95%')\n",
    "    print(f\"Overlap of Estimated pdf's at 95% confidence: {find_overlap_of_set(pdfs)}\")\n",
    "    \n",
    "    pdfs = [concrete_normal_dist(part.signals) for part in single_d_parts]\n",
    "    plot_single_distributions(pdfs, [f\"{part.type} - {part.sub_part_name}\" for part in single_d_parts], f'1D Analysis - Concrete')\n",
    "    print(f\"Overlap of concrete pdf's: {find_overlap_of_set(pdfs)}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "perform_1d_analysis(con_parts)\n",
    "#perform_1d_analysis(lid_parts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_overlap_of_set_with_meta_pdf(pdfs: List[NormalDistribution], samples: int, confidence_bound: float) -> float:\n",
    "    \"\"\"Estimates the overlap between a set of distributions.\n",
    "    \n",
    "    The meta pdf is really just the combined pdfs of all the distributions, then we're drawing from that\n",
    "    and seeing how many samples would cause conflicts. How can we prove the distribution we're pulling samples\n",
    "    from is representative of the entire population? Is the estimated confidence good enough? \n",
    "    \n",
    "    Could we potentially randomly sample from each distribution and just see which ones end up overlapping\n",
    "    with the other distributions? TODO (henry): Think about this more \n",
    "    \n",
    "    TODO (henry): This takes so long, like 5's per sample. We need to run 10,000's samples many times. \n",
    "    Need to computationall optimize this.\n",
    "    \"\"\"\n",
    "    \n",
    "    pdf_means = [pdf.mean for pdf in pdfs]    \n",
    "    min_confidence = 1 - confidence_bound\n",
    "    meta_pdf = estimate_normal_dist(pdf_means, 0.95)\n",
    "    meta_samples = np.random.multivariate_normal(meta_pdf.mean, np.diag(meta_pdf.std), samples)\n",
    "    mvn_pdfs = [mvn(mean=pdf.mean, cov=pdf.std) for pdf in pdfs]\n",
    "        \n",
    "    sample_confidences = [\n",
    "        [pdf.cdf(sample) for pdf in mvn_pdfs]\n",
    "        for sample in meta_samples]\n",
    "    \n",
    "    print(f\"Sample confidences: {sample_confidences}\")\n",
    "    filtered_confidences = [\n",
    "        list(filter(lambda confidence: confidence >= min_confidence, sample_confidence))\n",
    "        for sample_confidence in sample_confidences]\n",
    "\n",
    "    print(f\"Filtered confidences: {filtered_confidences}\")\n",
    "    # We're ok with up to 1 match, but every one more than that is a conflict.\n",
    "    collisions = [max(len(confidences)-1, 0) for confidences in filtered_confidences]\n",
    "    print(f\"Collisions: {collisions}\")\n",
    "    return sum(collisions)/samples\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_overlap_of_set_with_individual_pdf(pdfs: List[NormalDistribution], samples: int, confidence_bound: float) -> float:\n",
    "    \"\"\"Estimates the overlap between a set of distributions.\n",
    "    \n",
    "    Pull from the samples from each individual distribution and see how many samples would cause conflicts \n",
    "    with other distributions. \n",
    "    \n",
    "    Does this really represent the range of the entire population?\n",
    "    \n",
    "    TODO (henry): This takes so long, like 5's per sample per pdf. We need to run 10,000's samples many times. \n",
    "    Need to computationall optimize this.\n",
    "    \"\"\"\n",
    "    \n",
    "    min_confidence = 1 - confidence_bound\n",
    "    mvn_pdfs = [mvn(mean=pdf.mean, cov=pdf.std) for pdf in pdfs]\n",
    "    pdf_samples = [\n",
    "        np.random.multivariate_normal(pdf.mean, np.diag(pdf.std), samples) \n",
    "        for pdf in pdfs]\n",
    "    \n",
    "    # flatten the samples\n",
    "    pdf_samples = [sample for pdf_sample in pdf_samples for sample in pdf_sample]\n",
    "     \n",
    "    sample_confidences = [\n",
    "        [pdf.cdf(sample) for pdf in mvn_pdfs]\n",
    "        for sample in pdf_samples]\n",
    "    \n",
    "    filtered_confidences = [\n",
    "        list(filter(lambda confidence: confidence >= min_confidence, sample_confidence))\n",
    "        for sample_confidence in sample_confidences]\n",
    "    \n",
    "    \n",
    "    # We're ok with up to 1 match, but every one more than that is a conflict.\n",
    "    collisions = [1 for confidences in filtered_confidences if len(confidences) > 1]\n",
    "    return sum(collisions)/(samples*len(pdfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample confidences: [[0.09525208834585865, 0.0, 0.9847667939896262], [0.016129839766666966, 0.0, 0.20841639943640602], [2.283057122372941e-10, 0.0, 0.0701906556437915], [0.021802560449977446, 0.0, 0.00022190753091749047], [2.2254348318526624e-34, 0.0, 0.321405779571586]]\n",
      "Filtered confidences: [[0.09525208834585865, 0.9847667939896262], [0.20841639943640602], [0.0701906556437915], [], [0.321405779571586]]\n",
      "Collisions: [1, 0, 0, 0, 0]\n",
      "Estimated collision rate from meta pdf: 0.2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def perform_multivariant_analsis(parts: List[Part]):\n",
    "    \n",
    "    multivariant_parts = limit_deminsionality(parts, list(range(10)))\n",
    "    pdfs = [estimate_normal_dist(part.signals, 0.95) for part in multivariant_parts]\n",
    "    \n",
    "    estimated_meta_collision_rate = estimate_overlap_of_set_with_meta_pdf(pdfs, 5, 0.95)\n",
    "    print(f\"Estimated collision rate from meta pdf: {estimated_meta_collision_rate}\")\n",
    "    \n",
    "    # estimated_set_collision_rate = estimate_overlap_of_set_with_individual_pdf(pdfs, 100, 0.95)\n",
    "    # print(f\"Estimated collision rate from set of individual pdfs: {estimated_set_collision_rate}\")\n",
    "    \n",
    "perform_multivariant_analsis(con_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multivariant_parts = limit_deminsionality(con_parts, list(range(500)))\n",
    "pdfs = [estimate_normal_dist(part.signals, 0.95) for part in multivariant_parts]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "25a19fbe0a9132dfb9279d48d161753c6352f8f9478c2e74383d340069b907c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
